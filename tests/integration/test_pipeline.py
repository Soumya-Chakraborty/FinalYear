"""
Integration tests for the RaagHMM system.

Tests that verify the complete pipeline works together.
"""

import numpy as np
import tempfile
import json
from pathlib import Path
import pytest
from unittest.mock import patch

from src.raag_hmm.train.trainer import RaagTrainer
from src.raag_hmm.train.persistence import ModelPersistence
from src.raag_hmm.infer.classifier import RaagClassifier
from src.raag_hmm.pitch.extractor import extract_pitch_with_fallback
from src.raag_hmm.pitch.smoother import smooth_pitch
from src.raag_hmm.quantize.sequence import quantize_sequence


class TestCompletePipeline:
    """Integration tests for the complete RaagHMM pipeline."""
    
    def test_training_to_classification_pipeline(self, temp_dir):
        """Test the complete pipeline from training to classification."""
        # This test may be skipped if dependencies aren't available
        try:
            # Create a simple mock dataset directory structure
            dataset_dir = temp_dir / "dataset"
            train_dir = dataset_dir / "train"
            train_audio_dir = train_dir / "audio"
            train_meta_dir = train_dir / "metadata"
            
            train_audio_dir.mkdir(parents=True)
            train_meta_dir.mkdir(parents=True)
            
            # Create mock audio files and metadata
            mock_raags = ["Bihag", "Yaman", "Desh"]
            for i, raag in enumerate(mock_raags):
                # Create a mock audio file (empty file for this test)
                audio_file = train_audio_dir / f"{raag.lower()}_test_{i}.wav"
                audio_file.touch()
                
                # Create corresponding metadata
                metadata = {
                    "recording_id": f"{raag.lower()}_test_{i}",
                    "raag": raag,
                    "tonic_hz": 261.63 + i*10,  # Slightly different tonics
                    "artist": "Test Artist",
                    "instrument": "sitar",
                    "split": "train"
                }
                
                meta_file = train_meta_dir / f"{raag.lower()}_test_{i}.json"
                with open(meta_file, 'w') as f:
                    json.dump(metadata, f)
            
            # Initialize trainer
            trainer = RaagTrainer(
                n_states=12,
                n_observations=12,
                max_iterations=5,  # Few iterations for test
                convergence_tolerance=0.1
            )
            
            # Mock the audio processing methods to avoid actual audio processing
            def mock_extract_sequence(audio_path, tonic_hz, **kwargs):
                # Generate synthetic sequence for testing
                return np.array([0, 2, 4, 5, 7, 9, 11, 0, 2, 4])  # C major scale fragment
            
            trainer.extract_and_quantize_sequence = mock_extract_sequence
            
            # Train models
            trained_models = trainer.train_all_raag_models(
                str(dataset_dir),
                split="train",
                verbose=False
            )
            
            assert len(trained_models) == len(mock_raags)
            
            # Save models
            models_dir = temp_dir / "models"
            persistence = ModelPersistence(str(models_dir))
            saved_paths = persistence.save_all_models(trained_models)
            
            assert len(saved_paths) == len(mock_raags)
            
            # Load models and test classification
            classifier = RaagClassifier(str(models_dir))
            
            # Test with a mock sequence
            test_sequence = np.array([0, 2, 4, 5, 7, 9, 11])
            prediction = classifier.predict_raag(test_sequence)
            
            # Should return one of the trained raags
            assert prediction in mock_raags
            
            # Test with confidence
            result = classifier.predict_with_confidence(test_sequence)
            
            assert 'predicted_raag' in result
            assert 'confidence' in result
            assert 'scores' in result
            assert result['predicted_raag'] in mock_raags
            
        except Exception as e:
            pytest.skip(f"Integration test skipped due to dependency issues: {e}")
    
    def test_pitch_extraction_quantization_pipeline(self, mock_audio_data):
        """Test the pitch extraction â†’ smoothing â†’ quantization pipeline."""
        try:
            audio_data, sr = mock_audio_data
            
            # Extract pitch
            f0_hz, voicing_prob = extract_pitch_with_fallback(
                audio_data, sr, frame_sec=0.0464, hop_sec=0.01
            )
            
            # Smooth pitch
            smoothed_f0, updated_voicing = smooth_pitch(f0_hz, voicing_prob)
            
            # Quantize with a mock tonic
            tonic_hz = 261.63  # C4
            quantized_seq = quantize_sequence(smoothed_f0, tonic_hz)
            
            # Verify the output is a valid sequence
            assert len(quantized_seq) == len(smoothed_f0)
            assert quantized_seq.dtype in [np.int32, np.int64, int]
            
            # Valid chromatic bins should be in [0, 35] range for 36 bins
            valid_bins = quantized_seq[~np.isnan(quantized_seq)]
            if len(valid_bins) > 0:
                assert np.all(valid_bins >= 0) and np.all(valid_bins <= 35)
                
        except Exception as e:
            pytest.skip(f"Pipeline test skipped due to dependency issues: {e}")
    
    @pytest.mark.integration
    def test_model_persistence_roundtrip(self, temp_dir):
        """Test that models can be saved and loaded correctly."""
        try:
            # Create and train a simple model
            from src.raag_hmm.hmm.model import DiscreteHMM
            
            hmm = DiscreteHMM(n_states=5, n_observations=8, random_state=42)
            training_sequences = [np.array([0, 1, 2, 3, 4, 5, 6, 7])]
            
            # Train for a few iterations
            hmm.train(training_sequences, max_iterations=3, verbose=False)
            
            # Get original parameters
            original_pi, original_A, original_B = hmm.get_parameters()
            
            # Save model
            models_dir = temp_dir / "models"
            persistence = ModelPersistence(str(models_dir))
            model_path, meta_path = persistence.save_model("test_raag", hmm, {
                "raag_name": "test_raag",
                "n_sequences": 1,
                "total_frames": 8
            })
            
            # Load model
            loaded_model, loaded_metadata = persistence.load_model("test_raag")
            
            # Compare parameters
            loaded_pi, loaded_A, loaded_B = loaded_model.get_parameters()
            
            np.testing.assert_array_almost_equal(original_pi, loaded_pi)
            np.testing.assert_array_almost_equal(original_A, loaded_A)
            np.testing.assert_array_almost_equal(original_B, loaded_B)
            
            # Check metadata
            assert loaded_metadata['raag_name'] == "test_raag"
            assert loaded_metadata['model_class'] == "DiscreteHMM"
            
        except Exception as e:
            pytest.skip(f"Model persistence test skipped: {e}")


class TestErrorHandlingIntegration:
    """Test error handling across module boundaries."""
    
    def test_error_propagation(self):
        """Test that errors are properly propagated through the system."""
        # Test with invalid tonic frequency
        with pytest.raises(Exception):  # Should raise QuantizationError or similar
            quantize_sequence(np.array([220.0, 440.0]), tonic_hz=0.0)
        
        # Test with invalid HMM parameters
        with pytest.raises(Exception):  # Should raise ValueError or ModelTrainingError
            hmm = DiscreteHMM(n_states=0, n_observations=10)  # Invalid n_states